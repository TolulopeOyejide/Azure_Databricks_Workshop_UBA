{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb808893-430b-44fe-a883-a6675035af18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Connecting Databricks to Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9f48d25-1c4a-4131-9824-608cd70c3a85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the dbutils module from the Databricks runtime\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.dbutils import DBUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64e56c92-f12a-4cb6-a647-a3e9e760f377",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Initialize a SparkSession if you haven't already done so. This is required to interact with Spark.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Blob Storage Mount\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d44c6a62-5028-4d01-b91a-305d140adb35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Provide your Azure Blob Storage account credentials\n",
    "account_name = \"YOUR_ACCOUNT_NAME\"\n",
    "container_name = \"YOUR_CONTAINER_NAME\"\n",
    "access_key = \"YOUR_ACCESS_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b1e0c2a-00ee-457d-810c-8e2998135054",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount Blob Storage container\n",
    "dbutils.fs.mount(\n",
    "  source = f\"wasbs://{container_name}@{account_name}.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt/<mount_name>\",\n",
    "  extra_configs = {\"fs.azure.account.key.\"+account_name+\".blob.core.windows.net\": access_key}\n",
    ")\n",
    "#Replace <mount_name> with the desired mount point name. This will be the directory where your Blob Storage container will be mounted in the Databricks file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25a851fc-6e36-4012-9f94-0aff7351de58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unmount Blob Storage container (optional)\n",
    "dbutils.fs.unmount(\"/mnt/<mount_name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d925e95-aa5f-4b1b-9dd0-2654975d1093",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Connecting Databricks to ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97b09c36-8ea0-46c8-a342-2c3774bc6f6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define your ADLS Gen2 account details\n",
    "adls_account_name = \"YOUR_ADLS_ACCOUNT_NAME\"\n",
    "adls_container_name = \"YOUR_CONTAINER_NAME\"\n",
    "adls_client_id = \"YOUR_SERVICE_PRINCIPAL_CLIENT_ID\"\n",
    "adls_client_secret = \"YOUR_SERVICE_PRINCIPAL_CLIENT_SECRET\"\n",
    "adls_tenant_id = \"YOUR_TENANT_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f9b03ac-176d-4554-837a-f5e91fbe6ca9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount ADLS Gen2\n",
    "dbutils.fs.mount(\n",
    "  source = f\"abfss://{adls_container_name}@{adls_account_name}.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/<mount_name>\",\n",
    "  extra_configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": adls_client_id,\n",
    "    \"fs.azure.account.oauth2.client.secret\": adls_client_secret,\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{adls_tenant_id}/oauth2/token\"\n",
    "  }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3815979-ab1f-471f-8214-9b3b649bc36f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Replace the placeholders YOUR_ADLS_ACCOUNT_NAME, YOUR_CONTAINER_NAME, YOUR_SERVICE_PRINCIPAL_CLIENT_ID, YOUR_SERVICE_PRINCIPAL_CLIENT_SECRET, and YOUR_TENANT_ID with your actual ADLS Gen2 account details.\n",
    "\n",
    "Make sure to provide the appropriate permissions to the service principal associated with the client ID and client secret. The service principal should have sufficient permissions to access the ADLS Gen2 account and the specified container.\n",
    "\n",
    "Replace <mount_name> with the desired mount point name. This will be the directory where your ADLS Gen2 container will be mounted in the Databricks file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d03d0ad-443f-4b3b-9298-5998f674667c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unmount ADLS Gen2 (optional)\n",
    "dbutils.fs.unmount(\"/mnt/<mount_name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d77aee71-9a92-49e5-a8ed-70e6eed5dd61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark API for reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "338c12e3-2c11-494c-ae43-521af2012c97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a917c3f-a1db-4845-9ba5-833f76973a0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"path/to/csv/file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "660b49f7-0046-481d-b8f3-9eb0779d7205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"path/to/json/file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b95f21f-ddaa-4aec-95ce-033a8cfdfcac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"path/to/parquet/file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41bfc415-4b55-442a-a006-7ad673bb62df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.jdbc(url=\"jdbc:mysql://hostname:port/database\", table=\"table_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3803f3de-5ddb-4b0a-8ee5-5fd2dc9e7951",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host:port\").option(\"subscribe\", \"topic\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14c3951f-7375-45be-b8cd-46d2c64ea8e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c74f217-3ea7-4813-ad24-32de32e257d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "133bb434-130c-468e-9a7a-a16ec50f59f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pandas API for reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c71278f8-6afe-47f9-91c4-71e0395bb740",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ed7f92f-bfc7-49a4-a895-daf826b8ff21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"file.xlsx\", sheet_name=\"Sheet1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9716066-4b68-4810-8d2f-b09efeba52c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07185539-1435-453a-bec4-5689a5dae055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e5eed78-ba8d-41b6-9b8c-dfbb50494a40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_hdf(\"file.h5\", key=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1861dd21-a92d-423c-8abb-2201e1fabf36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///file.db')\n",
    "df = pd.read_sql(\"SELECT * FROM table_name\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff0d2e4a-1f00-48b2-8ad1-f33325396479",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e03deca6-f4f6-401b-a7ea-005d92f448dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing data to a relational database (e.g., Azure SQL Database)\n",
    "jdbc_url = \"jdbc:sqlserver://<host>:<port>;databaseName=<database>\"\n",
    "df.write.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"<table>\").save()\n",
    "\n",
    "# For Amazon RDS or Google Cloud SQL, replace the JDBC URL with the appropriate connection string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2897e557-83fa-4fbf-854e-1e9b31349da3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing data to a relational database using SQLAlchemy (assuming you have created a SQLAlchemy engine)\n",
    "import sqlalchemy\n",
    "\n",
    "engine = sqlalchemy.create_engine(\"dialect+driver://username:password@host:port/database_name\")\n",
    "df.to_sql(name='table_name', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e26006d-5221-4117-bda8-ec657c08ffc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing data to Delta Lake\n",
    "df.write.format(\"delta\").save(\"/mnt/delta_path\")\n",
    "\n",
    "# Writing data to Apache Parquet format\n",
    "df.write.format(\"parquet\").save(\"/mnt/parquet_path\")\n",
    "\n",
    "# For Apache Hudi, additional configuration is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb1e2405-a451-439c-8766-3dd0758ea0e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pandas does not have built-in support for writing directly to Delta Lake or Apache Hudi.\n",
    "# However, you can write the DataFrame to Parquet format, which can be consumed by Delta Lake or Apache Hudi.\n",
    "df.to_parquet(\"/local/path/to/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4180a91b-6db9-4cb8-aae7-30bf04fead59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing data to Apache Kafka\n",
    "df.write.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host:port\").save()\n",
    "\n",
    "# For AWS Kinesis or Azure Event Hubs, additional configuration is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d107b02-2589-4b25-84f9-2791caad8c1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Processing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c741d8d1-a6b5-479b-8227-be27a41dee0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from a CSV file\n",
    "df = spark.read.csv(\"dbfs:/path/to/data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfe379ec-a0a8-4260-ab56-dda5e3ca2109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DATA EXPLORATION\n",
    "# Display the first few rows of the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Display summary statistics of numerical columns\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d062435-8848-4c19-8f45-7dc3cfe3ad87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DATA CLEANING\n",
    "# Drop rows with missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Fill missing values with a specified value\n",
    "df_filled = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10273204-71d3-4e1a-983d-8f63a820c85b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DATA TRANSFORMATION AND MANIPULATION\n",
    "# Filter data based on a condition\n",
    "df_filtered = df.filter(df[\"age\"] > 18)\n",
    "\n",
    "# Select specific columns\n",
    "df_selected = df.select(\"name\", \"age\")\n",
    "\n",
    "# Group data and compute aggregates\n",
    "df_grouped = df.groupBy(\"gender\").agg({\"salary\": \"avg\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d9a8c14-17ef-4ee2-83a8-624d0c840237",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Create a new column based on a condition\n",
    "df_with_new_column = df.withColumn(\"is_adult\", when(col(\"age\") >= 18, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72999b71-b4c9-4140-abdc-15ccef11278a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STASTITICAL ANALYSIS\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = Correlation.corr(df.select(\"age\", \"salary\"), method=\"pearson\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a65d5932-8153-453c-a593-1e0502677c77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DATA VISUALIZATION \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of ages\n",
    "ages = df.select(\"age\").rdd.flatMap(lambda x: x).collect()\n",
    "plt.hist(ages, bins=20)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Ages\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea956db2-11ff-473c-a9e9-926b4e733f0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Linking  Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "179a0a05-7f1f-47eb-991a-01e04cd1ef52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Master Notebook\n",
    "\n",
    "# Run data extraction notebook\n",
    "%run \"/Workspace/extract_notebook\" parameter1=value1 parameter2=value2\n",
    "\n",
    "# Run data transformation notebook\n",
    "%run \"/Workspace/transform_notebook\" parameter3=value3 parameter4=value4\n",
    "\n",
    "# Run data loading notebook\n",
    "%run \"/Workspace/load_notebook\" parameter5=value5 parameter6=value6"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Azure Integrations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
